{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "understanding-delhi",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "* Spark Core\n",
    "\n",
    "* Spark Core 扩展\n",
    "\n",
    "* SparkSQL\n",
    "\n",
    "* SparkStreaming\n",
    "\n",
    "Spark-Core是Spark最核心的包，是整个Spark的基础，提供了分布式任务调度和基本的I/O功能。\n",
    "\n",
    "Spark提供了批处理(RDDs)，结构化查询(SparkSQL   DataFrame)，流计算(SparkStreaming)，机器学习(MLlib)，图计算(GraphzX)等组件，这些组件都是依托于通用的计算引擎RDDs，所以Spark-Core的RDDs是整个Spark的基础。\n",
    "\n",
    "\n",
    "特点：\n",
    "\n",
    "* 速度快，在内存运行比MapReduce快100倍，在硬盘运行比MapReduce快10倍\n",
    "\n",
    "* 可以兼容现有大多数集群工具，YARN，Mesos，Kubernets，Spark Standalone，可以访问HBase，Hive，HDFS，Cassndra等\n",
    "\n",
    "* 提供80多种运算符\n",
    "\n",
    "* 提供全栈服务，并且支持JAVA，Scala，Python，R，SQL等语言的API\n",
    "\n",
    "\n",
    "Spark可以将程序运行在两种模式下：\n",
    "\n",
    "* 单机，通过线程模拟并行来运行程序\n",
    "\n",
    "* 集群，使用集群管理工具将程序运行在集群上\n",
    "\n",
    "\n",
    "## Spark 集群\n",
    "\n",
    "Spark如何将一个程序运行在一个集群上？\n",
    "\n",
    "要想管理数以千计的机器集群，有必要使用集群管理工具，比如Yarn，Mesos，Kubernets，Standalone。\n",
    "\n",
    "流程是，首先使用Spark的Client提交任务给集群管理工具申请资源，然后将计算任务分发到集群中运行。\n",
    "\n",
    "Client --> Cluster Manager --> Cluster\n",
    "\n",
    "Client 提交程序给Driver program, 创建SparkContext,该进程调用Spark程序的main方法，并且启动SparkContext\n",
    "\n",
    "Cluser 包含许多Woker Node进程，Worker是一个守护进程，负责启动和管理Executor, Executor负责运行Spark Task\n",
    "\n",
    "总结即Driver将程序转化为Tasks，并提交给Executor执行  \n",
    "\n",
    "\n",
    "Driver 和 Worker在什么时候启动？在不同的集群管理工具中表现不同。\n",
    "\n",
    "* Standalone\n",
    "\n",
    "  Standalone集群中分为两个角色，Master和Slave(Worker)，在Standalone集群中，启动之初就会创建固定数量的Worker。Driver的启动分为两种模式，Client和Cluster。在Client模式中，Driver运行在Client端，在Client启动时被启动；在Cluster模式中，Driver运行在某个Worker中，随应用的提交而启动。 \n",
    "\n",
    "* Yarn\n",
    "\n",
    "  在Yarn集群中也分为Client和Cluster模式。Cluster模式中，首先会和ResourceManager交互，开启ApplicationMaster，其中运行了Driver，Driver创建基础环境后，会由RM提供对应的容器，运行Executor，Executor会反向向Driver注册自己，并申请Tasks执行。 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-reference",
   "metadata": {},
   "source": [
    "## Spark集群环境配置\n",
    "\n",
    " 在清华国内源下载了spark-3.1.1\n",
    "\n",
    " 1. 安装：\n",
    "\n",
    " ```\n",
    " cd /usr/local/softwares/\n",
    " rz -E\n",
    " tar -xzvf spark-3.1.1 -C ../servers/\n",
    " mv spark-3.1.1 spark-3.1.1\n",
    " ```\n",
    "\n",
    " 2. 配置 JAVA 路径， Master 路径：\n",
    "\n",
    " ```shell\n",
    " cd spark-3.1.1\n",
    " ls\n",
    " cd conf\n",
    " mv spark-env.sh.template spark-env.sh\n",
    " vim spark-env.sh\n",
    " ```\n",
    "\n",
    " ​\t添加以下：\n",
    "\n",
    " ```\n",
    " #指定 JAVA Home\n",
    " export JAVA_HOME=/usr/local/servers/jdk1.8.0_271\n",
    " #指定 Spark Master 地址\n",
    " export SPARK_MASTER_HOST=node01\n",
    " export SPARK_MASTER_PORT=7077\n",
    " ```\n",
    "\n",
    " ​\t配置slaves：\n",
    "\n",
    " ​\t指定从节点，从而使用`sbin/start-all.sh`启动集群的时候，可以一键启动整个集群所有的worker\n",
    "\n",
    " ```shell\n",
    " cd /usr/local/servers/spark-3.1.1\n",
    " ls\n",
    " cd conf\n",
    " cp workers.template workers\n",
    " vim workers\n",
    " ```\n",
    "\n",
    " ```\n",
    " node01\n",
    " node02\n",
    " node03\n",
    " ```\n",
    "\n",
    " ​\t配置 HistoryServer：\n",
    "\n",
    " ​\t默认情况下，Spark程序运行完毕后，就无法查看运行记录的Web UI了，通过配置HistoryServer可以提供一个服务，通过读取日志文件，使得我们在程序运行结束后，依然能够查看运行过程。\n",
    "\n",
    " ```shell\n",
    " cd /usr/local/servers/spark-3.1.1\n",
    " ls\n",
    " cd conf\n",
    " cp spark-defaults.conf.template spark-defaults.conf\n",
    " vim spark-defaults.conf\n",
    " ```\n",
    "\n",
    " 在最后添加以下：\n",
    "\n",
    "     ```\n",
    "     spark.eventLog.enabled true\n",
    "     spark.eventLog.dir hdfs://node01:8020/spark_log\n",
    "     spark.eventLog.compress true\n",
    "     ```\n",
    "\n",
    "     在spark-env.sh中添加HistoryServer启动参数\n",
    "\n",
    "     ```\n",
    "     # 指定 Spark History 运行参数\n",
    "     export SPARK_HISTORY_OPTS=\"-Dspark.history.ui.port=4000 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://node01:8020/spark_log\"\n",
    "\n",
    "     ```\n",
    "\n",
    " ​\tTODO 为Spark创建HDFS中的日志目录\n",
    "\n",
    "     ```shell\n",
    "     hdfs dfs -mkdir -p /spark_log\n",
    "     ```\n",
    "\n",
    " 3. 分发\n",
    "\n",
    "    将Spark安装包分发给集群中其它机器\n",
    "\n",
    "    ```\n",
    "    cd /usr/local/servers/\n",
    "    scp -r spark-3.1.1 root@node02:$PWD\n",
    "    scp -r spark-3.1.1 root@node03:$PWD\n",
    "    ```\n",
    "\n",
    " 4. 启动Spark Master 和 Slaves，以及 HistoryServer\n",
    "\n",
    "    ```shell\n",
    "    cd /usr/loca/servers/spark-3.1.1\n",
    "    sbin/start-all.sh\n",
    "    sbin/start-history-server.sh\n",
    "    ```\n",
    "\n",
    "    查看进程\n",
    "\n",
    "    ```\n",
    "    jps\n",
    "    ```\n",
    "\n",
    "    在unix系统中，ps可以查看进程状态process status，有哪些进程以及进程id。\n",
    "\n",
    "    jps是java提供的一个可以查看所有java进程pid的命令。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-acoustic",
   "metadata": {},
   "source": [
    "**系统环境变量配置 ~/.bash_profile**\n",
    "\n",
    "\n",
    "```\n",
    "# Java11 path\n",
    "export JAVA_11_HOME=/usr/local/java/jdk-11.0.2.jdk/Contents/Home\n",
    "# Java8 path\n",
    "export JAVA_8_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_271.jdk/Contents/Home\n",
    "export JAVA_HOME=$JAVA_11_HOME\n",
    "alias jdk11=\"export JAVA_HOME=$JAVA_11_HOME\"\n",
    "alias jdk8=\"export JAVA_HOME=$JAVA_8_HOME\"\n",
    "export PATH=$JAVA_HOME/bin:$PATH:.\n",
    "# Maven\n",
    "export M3_HOME=/usr/local/maven/apache-maven-3.6.3\n",
    "export PATH=$M3_HOME/bin:$PATH\n",
    "# jenv\n",
    "export PATH=\"$HOME/.jenv/bin:$PATH\"\n",
    "eval \"$(jenv init -)\"\n",
    "\n",
    "#Spark path\n",
    "export SPARK_HOME=/usr/local/spark\n",
    "export PATH=$PATH:$SPARK_HOME/bin\n",
    "export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9-src.zip:$PYTHONPATH\n",
    "export PYSPARK_PYTHON=python3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-river",
   "metadata": {},
   "source": [
    "## Spark集群高可用搭建\n",
    "\n",
    "使用Zookeeper实现Spark Standalone高可用：\n",
    "\n",
    "对于Spark Standalone集群来说，当worker调度出现问题时，会自动的弹性容错，将出错的Task调度到其它woker执行。但是对于Master来说，可能会出现单点失败问题。Spark提供了两种方式满足高可用：\n",
    "\n",
    "- 使用Zookeeper实现Masters的主备切换\n",
    "- 使用文件系统做主备切换，此类场景很小\n",
    "\n",
    "1. 停止Spark集群\n",
    "\n",
    "```\n",
    "cd /usr/local/severs/spark-3.1.1/\n",
    "ls\n",
    "cd sbin\n",
    "stop-all.sh\n",
    "```\n",
    "\n",
    "2. 修改配置文件，指定Zookeeper路径，增加Spark运行时参数\n",
    "\n",
    "```\n",
    "cd /usr/local/severs/spark-3.1.1/\n",
    "ls\n",
    "cd conf\n",
    "vim spark-env.sh\n",
    "```\n",
    "\n",
    "注释掉SPARK_MASTER_HOST地址这一行, 添加Spark启动参数\n",
    "\n",
    "```\n",
    "# export SPARK_MASTER_HOST=node01\n",
    "\n",
    "# 指定 spark 运行时参数\n",
    "export SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node01:2181,node02:2181,node03:2181 -Dspark.deploy.zookeeper.dir=/spark\"\n",
    "```\n",
    "\n",
    "3. 分发配置文件到整个集群\n",
    "\n",
    "```\n",
    "cd /usr/local/severs/spark-3.1.1/conf\n",
    "scp spark-env.sh node02:$PWD\n",
    "scp spark-env.sh node03:$PWD\n",
    "```\n",
    "\n",
    "4. 启动\n",
    "\n",
    "在node01上启动整个集群\n",
    "\n",
    "```\n",
    "cd /usr/loca/servers/spark-3.1.1\n",
    "sbin/start-all.sh\n",
    "sbin/start-history-server.sh\n",
    "```\n",
    "\n",
    "在node02上单独再启动一个Master\n",
    "\n",
    "```\n",
    "cd /usr/loca/servers/spark-3.1.1\n",
    "sbin/start-master.sh\n",
    "```\n",
    "\n",
    "5. 查看node01 master 和 node02 master 的 Web UI\n",
    "\n",
    "一个为Alive(主),一个为Standby(备)\n",
    "\n",
    "Spark各服务端口：\n",
    "\n",
    "Master WebUI port: node01:8080\n",
    "\n",
    "Worker WebUI port: node01:8081\n",
    "\n",
    "History Server port: node01:4000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-pavilion",
   "metadata": {},
   "source": [
    "## Spark程序运行\n",
    "\n",
    "1. 进入spark安装目录\n",
    "\n",
    "```\n",
    "cd /usr/local/severs/spark-3.1.1/\n",
    "```\n",
    "\n",
    "2. 运行spark示例任务\n",
    "\n",
    "```\n",
    "bin/spark-submit \\   #spark-submit 提交命令\n",
    "--class org.apache.spark.examples.SparkPi \\    #指定jar包中运行的类\n",
    "--master spark://node01:7077,node02:7077,node03:7077 \\    #指定集群master地址，因为已经搭建了高可用，把几个节点都写上\n",
    "--executor-memory 1G \\\n",
    "--total-executor-core 2 \\\n",
    "/usr/local/severs/spark-3.1.1/examples/jars/spark-examples_2.11-2.2.3.jar \\    #指定jar包的位置\n",
    "100  #指定运行的函数参数\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "bin/pyspark --master spark://node01:7077\n",
    "```\n",
    "\n",
    "```\n",
    "bin/spark-shell --master spark://node01:7077\n",
    "```\n",
    "\n",
    "\n",
    "编写spark程序的两种方式：\n",
    "\n",
    "1. spark-shell \n",
    "\n",
    "    适用于数据集的探索，测试\n",
    "    \n",
    "    spark-shell 的原理是把每一行scala代码编译成类，最后交由spark执行\n",
    "    \n",
    "    启动spark-shell：\n",
    "    \n",
    "    进入spark安装目录，执行`spark-shell --master master`就可以提交Spark任务\n",
    "    \n",
    "    master地址的设置：\n",
    "\n",
    "    | master地址        | 说明                                                         |\n",
    "    | ----------------- | ------------------------------------------------------------ |\n",
    "    | local[N]          | 使用N条worker线程在本地运行                                  |\n",
    "    | spark://host:port | 在spark standalone中运行，指定spark集群的master地址，端口默认为7077 |\n",
    "    | mesos://host:port | 在apache mesos中运行，指定mesos地址                          |\n",
    "    | yarn              | 在yarn中运行，yarn地址由环境变量HADOOP_HADOOP_CONF_DIR指定   |\n",
    "    \n",
    "    \n",
    "**读取本地文件**\n",
    "\n",
    "```\n",
    "rdd = sc.textFile(\"file:///usr/local/wordcount.txt\")\n",
    "```\n",
    "\n",
    "**读取HDFS文件三种方式**\n",
    "\n",
    "```\n",
    "rdd = sc.textFile(\"hdfs://node01:8020/dataset/wordcount.txt\")\n",
    "\n",
    "rdd = sc.textFile(\"hdfs:///dataset/wordcount.txt\")\n",
    "\n",
    "rdd = sc.textFile(\"/dataset/wordcount.txt\")\n",
    "```\n",
    "\n",
    "\n",
    "2. spark-submit\n",
    "\n",
    "    适用于独立应用，上线集群 运行\n",
    "    \n",
    "    \n",
    "**独立应用运行的两种方式**\n",
    "\n",
    "1. 本地运行\n",
    "\n",
    "2. 提交运行\n",
    "\n",
    "   修改代码 -> 打包上传 -> bin/spark-submit 集群运行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-yacht",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "RDD是一个弹性分布式数据集 \n",
    "\n",
    "RDD 特点：\n",
    "\n",
    "* RDD是数据集\n",
    "\n",
    "* RDD是编程模型\n",
    "\n",
    "* RDD之间可以相互依赖\n",
    "\n",
    "* RDD是可以分区的\n",
    "\n",
    " \n",
    "SparkContext是Spark-Core的入口组件，是Spark程序的入口。如果把Spark程序分为前后端，服务端就是运行Spark程序的集群，前端就是Driver，在Driver中，SparkContext是最主要的组件，是Driver的核心，Driver运行时首先创建的组件。\n",
    "\n",
    "SparkContext主要作用是连接集群，创建RDD，累加器，广播变量等。 \n",
    "\n",
    "创建RDD的三种方式：\n",
    "\n",
    "* 从本地创建\n",
    "\n",
    "```\n",
    "rdd = sc.parallelize(seq, numSlices)\n",
    "```\n",
    "\n",
    "分区数由自己指定\n",
    "\n",
    "* 从文件创建\n",
    "\n",
    "```\n",
    "rdd = sc.textFile(\"hdfs:///dataset/..\")\n",
    "```\n",
    "\n",
    "路径也可以是file:// ，如果路径是hdfs，则rdd的分区数等于hdfs文件的block数量，即一个block就是一个分区。\n",
    "\n",
    "* 从其它RDD衍生\n",
    "\n",
    "\n",
    "在RDD上执行算子操作生成新的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lucky-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install findspark\n",
    "#pip install py4j\n",
    "#Py4j可以使运行于python解释器的python程序动态的访问java虚拟机中的java对象\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exotic-round",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 12, 13, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"miniProject\").setMaster(\"local[*]\")\n",
    "sc=SparkContext.getOrCreate(conf)\n",
    "rdd=sc.parallelize([1,2,3,4,5])\n",
    "rdd1=rdd.map(lambda r:r+10)\n",
    "print(rdd1.collect())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-margin",
   "metadata": {},
   "source": [
    "## Ref\n",
    "\n",
    "https://fuhailin.github.io/Spark-Tutorial/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
