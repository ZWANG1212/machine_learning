## P13 Clustering

**K Means Algorithm**

K Means是应用最广泛的聚类算法。

Input:

* K (number of clusters)
* Training set {$x^{(1)}, x^{(2)},…, x^{(m)}$}, $x^{(i)} \epsilon R^n$ (drop $x_0 = 1$ convention)

K-means optimization objective

$c^{(i)}$ = index of cluster (1, 2,…, K) to which example $x^{(i)}$ is currently assigned.

$u_k$ = cluster centroid $k$ ($u_k \epsilon R^n$)

$u_c(i)$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned

Optimization objective:
$$
J(c^{(1)}, ..., c^{(m)}, u_1,...u_K) = \frac{1}{m} \sum_{i=1}^m \lVert x^{(i) - u_{c^{(i)}}} \rVert^2
$$

$$
\underset{\underset{u_1,...u_K}{c^{(1)}, ..., c^{(m)},}}{min} J(c^{(1)}, ..., c^{(m)}, u_1,...u_K)
$$

质心：均值，向量各维取平均

距离：常用欧几里得距离或余铉相似度，但向量的各纬度需要提前正则化

在实际应用中，通常是手动选择聚类数量。

choosing the value of K:

Elbow method 肘部法则:

通过选取K为1, 2, 3, ...，随着K的增大，Cost function J 不断变小，绘制Cost functin J 和 K的关系曲线，曲线明显拐弯的地方就是Elbow点的K值。

实际上，肘部法则并不经常使用，因为实际运用到聚类问题时，代价函数与K值的关系曲线可能比较模糊，无法找到肘部。

实际上，当我们为了later/downstream purpose运用K-means算法时，评估K-means算法是基于how well it performs for the later purpose.

## P14 Dimensionality Reduction 降维

目的：1. Data compression 数据压缩，提高算法速度；2. 数据可视化

例如，当有两个feature是高度相关的，可能就需要去掉一个feature来降低维数。

2D->1D压缩，把平面上的数据点投影到一条线上。

3D->2D压缩，把空间中的数据点投影到一个平面上。

Principal Component Analysis (PCA) problem formulation

Reduce from 2D to 1D: Find a direction (a vector $u^{(1)}$) onto which to project the data so as to minimize the projection error.

Reduce from n-D to k-D: Find k vectors ($u^{(1)}, u^{(2)}, …, u^{(k)}$) onto which to project the data, so as to minimize the projection error.

projection error ：原数据点和对应投影点的距离 

**Data preprocessing**

Training set: $x^{(1)}, x^{(2)}, … ,x^{(m)}$

Preprocessing (feature scaling/mean normalization):
$$
u_j = \frac{1}{m} \sum_{i=1}^m x_j^{(i)}
$$
得到均值

Replace each $x_j^{(i)}$ with $x_j - u_j$ , 每个数据值减去均值

If different features on different scales (e.g. $x_1$ = size of house, $x_2$ = number of bedrooms), scale features to have comparable range of values.
$$
x_j^{(i)} := \frac{x_j^{(i)} - u_j}{s_j}
$$
$s_j$ 可以是feature j的最大值-最小值，或者更常用的是特征j的标准差standard deviation.

**PCA algorithm**

Reduce data from n-D to k-D

Computer "covariance matirx" 协方差矩阵:
$$
\Sigma = \frac{1}{m} \sum_{i=1}^n x^{(i)} (x^{(i)})^T
$$
$x^{(i)}$ 的维度是(n, 1)，$(x^{(i)})^T$ 的维度是(1, n)，所以 $\Sigma$ 是一个nxn的矩阵 

Compute "eigenvectors" of matrix $\Sigma$:
$$
[U, S, V] = svd(Sigma)
$$
svd: singular value decomposition，奇异值分解，主流语言的线性代数的包里都有这个函数

U matrix也是一个nxn的矩阵，前k列向量就是在k个方向上的投影

From $[U, S, V] = svd(Sigma)$ , we get:
$$
U = \left[ u^{(1)} u^{(2)} ... u^{(n)}\right] \epsilon R^{n \text{x} n}
$$
Then
$$
U_{reduced} = \left[ u^{(1)} u^{(2)} ... u^{(k)}\right] \epsilon R^{n \text{x} k}
$$

$$
\begin{equation}
\begin{aligned}
Z &= U_{reduced}^T X \\
   &= \begin{bmatrix} u^{(1)} \\ u^{(2)} \\ \vdots \\ u^{(k)} \end{bmatrix} X
\end{aligned}
\end{equation}
$$

$U_{reduced}^T$ 维度是(k, n), $X$ 维度是(n, 1)，所以$Z$ 的维度是(k, 1)

PCA算法总结：

 After mean normalization (ensure every feature has 0 mean) and optional feature scaling:

$\Sigma = \frac{1}{m} \sum_{i=1}^n x^{(i)} (x^{(i)})^T$

$[U, S, V] = svd(Sigma)$

$U_{reduced} = U(:, 1:K)$

$Z = U_{reduced}^{'} * X$

**Chosing the number of principal components**

Average squared projection error:
$$
\frac{1}{m} \sum_{i=1}^m \Vert x^{(i)} - x_{approx}^{(i)} \Vert^2
$$
Total variation in the data:
$$
\frac{1}{m} \sum_{i=1}^m \Vert x^{(i)} \Vert^2
$$
Typically, choose k to be smallest value so that
$$
\frac{\frac{1}{m} \sum_{i=1}^m \Vert x^{(i)} - x_{approx}^{(i)} \Vert^2}{\frac{1}{m} \sum_{i=1}^m \Vert x^{(i)} \Vert^2} \leq 0.05  \text{or}  0.01
$$
meaning 95% or 99% of variance is retained.

**Choosing k**

Algorithm:

for k = 1, … 

​	Compute $U_{reduced}, z^{(1)}, z^{(2)}, …, z^{(m)}, x_{approx}^{(1)}, …, x_{approx}^{(m)}$

​	Check if 
$$
\frac{\frac{1}{m} \sum_{i=1}^m \Vert x^{(i)} - x_{approx}^{(i)} \Vert^2}{\frac{1}{m} \sum_{i=1}^m \Vert x^{(i)} \Vert^2} \leq 0.05  \text{or}  0.01
$$
until find k to satisfy the above equation.

但是上面的方法效率太低了，更有效的方法是利用$[U, S, V] = svd(Sigma)$中的S矩阵：
$$
s = \begin{bmatrix} s_{11} &  &  &  &\\  &  s_{22} & & & \\ & &  s_{33} & & \\ & & & \ddots & & \\ & & & & s_{nn}\end{bmatrix}
$$
pick smallest k to satify:
$$
1-\frac{\sum_{i=1}^k s_{ii}}{\sum_{i=1}^n s_{ii}} \leq 0.05 or 0.01
$$
or:
$$
\frac{\sum_{i=1}^k s_{ii}}{\sum_{i=1}^n s_{ii}} \geq 0.95 or 0.99
$$
**Reconstruction from compressed representation**

由$Z = U_{reduced}^T X \\$ 
$$
X \approx X_{approx} = U_{reduced} Z
$$
$Z$ 的维度是(k, 1), $U_{reduced}$ 的维度是(n, k), 所以X的维度是(n, 1)

**PCA 应用**

* Supervised learning speedup

  例如，我们有数据集

  ($x^{(1)}, y^{(1)}$), ($x^{(2)}, y^{(2)}$),..., ($x^{(m)}, y^{(m)}$)

  x为100x100的图像，特征维度就是(10000, 1)

  Extract inputs:

  ​	Unlabeled dataset: $x^{(1)}$, $x^{(2)}$,..., $x^{(m)}$ $\epsilon R^{10000}$ 

  ​	—> PCA —>

  ​	$z^{(1)}$, $z^{(2)}$,..., $z^{(m)}$ $\epsilon R^{1000}$ 

  New training set:

  ($z^{(1)}, y^{(1)}$), ($z^{(2)}, y^{(2)}$),..., ($z^{(m)}, y^{(m)}$)

* 可视化

* 减少存储数据的空间

如果意图使用PCA减少特征数量，从而减少overfit的可能性，这个方法并不好，因为PCA会丢失一些有价值的信息，最好还是使用正则化regularization来解决overfit问题。
$$
\underset{\theta}{min} \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2 + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2
$$

## P15

**高斯(正态)分布**

概率密度函数：
$$
p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$
$\mu$ 均值，$\sigma$ 标准差

Parameter estimation:

给定数据集: {$x^{(1)}, x^{(2)},…, x^{(m)}$}

则：$\mu = \frac{1}{m}\sum_{i=1}^m x^{(i)}$,

$\sigma^2 = \frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)^2$ 

Density estimation:

如果x的特征满足：

$x_1 \backsim N(\mu_1, \sigma_1^2)$, $x_2 \backsim N(\mu_2, \sigma_2^2)$, $x_3 \backsim N(\mu_3, \sigma_3^2)$, ... , $x_n \backsim N(\mu_n, \sigma_n^2)$

则x的概率密度：

$P(x) = p(x_1; \mu_1, \sigma_1^2)p(x_2; \mu_2, \sigma_2^2)p(x_3; \mu_3, \sigma_3^2) = \Pi_{j=1}^np(x_j; \mu_j, \sigma_j^2)$

**Anomaly detection algorithm**

1. Choose features $x_i$ that might be indicative of anomalous examples.

2. Fit parameters $\mu_1, …, \mu_n, \sigma_1^2, …, \sigma_n^2$
   $$
   \mu_j = \frac{1}{m}\sum_{i=1}^mx_j^{(i)}
   $$

   $$
   \sigma_j^2 = \frac{1}{m}\sum_{i=1}^m(x_j^{(i)}-\mu_j)^2
   $$

   这里也都可以向量化计算。

3. Given new example x, compute p(x):
   $$
   \begin{aligned}p(x) &= \Pi_{j=1}^n p(x_j; \mu_j, \sigma_j^2) \\ &= \Pi_{j=1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2}) \end{aligned}
   $$
   ​

   Anomaly if $p(x) < \varepsilon$

**Algorithm evaluation**

Fit model $p(x)$ on training set ($x^{(1)}, y^{(1)}$), ($x^{(2)}, y^{(2)}$),..., ($x^{(m)}, y^{(m)}$)

On a cross validation/test example $x$, predict
$$
y = \begin{cases} 1 & \text{if} p(x)<\varepsilon \text{(anomaly)} \\ 0 & \text{if} p(x) \geq \varepsilon \text{(normal)}  \end{cases}
$$

Possible evaluation metrics:

 - True positive, false positive, false negative, true negative

- Precision/Recall

- $F_1$-score

  Can also use cross validation set to choose parameter $\varepsilon$.

**Anomaly detection vs. Supervised learning**

Anomaly detection:

Very small number of positive examples (0-20 is common)

Large number of negative examples.

Supervised learning:

Large number of positive and negative examples.

**Multivariate gaussian distuibution 多元高斯分布**  

多元高斯模型中不是对 $p(x_1), p(x_2),...$ 分别建模，而是对所有的$p(x)$ 一次建模。

$$
   p(x) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} exp(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu))
$$

其中, $\mu \epsilon R^n$, $\Sigma \epsilon R^{n\text{x}n}$ (协方差矩阵), $|\Sigma|$ 矩阵行列式 

多元高斯模型最大的优势就是通过$\Sigma$矩阵的对角线两边的数值能够描述两个特征变量之间可能存在正相关或负相关。
比如：
$$
\Sigma= \begin{bmatrix} 1 & -0.5 \\ -0.5 & 1 \end{bmatrix}
$$

表明两个特征向量是负相关。

**Anomaly detection with the multivariate Gaussian**

1. Fit model $p(x)$ by setting

   $$
   \mu = \frac{1}{m} \sum_{i=1}^m x^{(i)}
   $$

   $$
   \Sigma = \frac{1}{m} \sum_{i=1}^m (x^{(i)}-\mu)(x^{(i)}-\mu)^T
   $$

2. Given a new example $x$, compute
   $$
   p(x) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} exp(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu))
   $$
   Flag an anomaly if $p(x) < \varepsilon$


**Relationship to original model**
Original model:
$$
P(x) = p(x_1; \mu_1, \sigma_1^2)p(x_2; \mu_2, \sigma_2^2)...p(x_n; \mu_n, \sigma_n^2)
$$
Corresponds to multivariate Gaussian
$$
p(x; \mu, \Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}} exp(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu))
$$
where
$$
\Sigma = \begin{bmatrix}  \sigma_{1} &  &  &  &\\  &  \sigma_{2} & & & \\ & &  \sigma_{3} & & \\ & & & \ddots & & \\ & & & & \sigma_{n} \end{bmatrix}
$$
即原模型只是对应多元高斯模型的一种特殊情况，多元高斯模型只在$\Sigma$ 矩阵对角线两侧都是0时才是原模型。

原始模型计算量小，可以适应大规模特征数量，多元高斯模型因为需要计算$\Sigma$ 的逆矩阵，计算量很大。

原始模型即使训练样本很少也可以，多元高斯模型必须样本数量m>特征数量n,不然$\Sigma$会是奇异矩阵，不可逆的。一般来说m>=10n.如果有冗余特征，比如两个特征高度相关，也会导致矩阵不可逆。

原始模型需要手动设计一些新异常特征，比如$x_3 = \frac{x_1}{x_2}$,而多元高斯模型可以自动捕获特征之间的关系。