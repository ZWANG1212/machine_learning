泛化：一个已经训练好的模型，在新的样本集上还能正确分类的能力。

原始输入向量通常会被进行预处理(pre-processed)，这个过程被称为特征提取(feature extraction)。测试数据必须与训练数据使用相同的预处理过程。

Famous Activation Functions

1. Identity
   $$
   f(x) = x
   $$

2. Binary Step 阶跃函数
   $$
   f(x) = \begin{cases} 0, & \text{for x < 0} \\ 1, & \text{for x $\geq$ 0} \end{cases}
   $$
   ​

3. Logistic or Sigmoid
   $$
   f(x) = \frac{1}{1+e^{-x}}
   $$

   $$
   f(x) \epsilon (0, 1)
   $$

   优点：便于求导；能压缩数据；适用于前向传播

   缺点：

   * 容易出现梯度消失，当激活函数处于饱和区时，导数接近0，无法完成深度神经网络训练
   * 函数输出不是0均值，容易造成梯度总是往一个方向更新，收敛缓慢
   * 指数运算比较耗时

4. Tanh
   $$
   f(x) = tanh(x) = \frac{2}{1+e^{-2x}}-1
   $$

   $$
   f(x) \epsilon (-1, 1)
   $$

   优点：0均值

   缺点：梯度消失；指数运算耗时

5. ArcTan
   $$
   f(x) = tan^{-1}(x)
   $$

   ​
   $$
   f(x) \epsilon (-\frac{\pi}{2}, \frac{\pi}{2})
   $$

6. Rectified Linear Unit (ReLU)

$$
f(x) = \begin{cases} 0,  & \text{for $x<0$} \\ x, & \text{for $x \geq 0$} \end{cases}
$$
​	优点：
	1. 收敛速度快，梯度不会饱和，解决了梯度消失问题
	2. 计算复杂度低，不需要指数运算
	3. 适合用于反向传播
	缺点：
	1. ReLU输出不是0均值
	2. 参数初始化时learning rate设置过高，参数更新太大，导致有些神经元坏死，永远不会被更新（在负数部分，梯度为0）
	3. 不会对数据做幅值压缩，会导致随着模型层数增加而不断扩张。

7. Leaky ReLU
   $$
   f(x) = \begin{cases} 0.01x,  & \text{for $x<0$} \\ x, & \text{for $x \geq 0$} \end{cases}
   $$
   ​
	优点：解决ReLU神经元坏死问题，但是表现不一定会比ReLU好

8. Softmax

$$
\sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}, for j=1,..., K.
$$

​	softmax应用于多分类器中，将多个神经元的输出映射到(0, 1)区间内，作为概率来进行多分类。

![img](https://pic1.zhimg.com/80/v2-87b232ab0e292a536e94b73952caadd0_1440w.jpg?source=1940ef5c)

​	优点：梯度求导非常方便。