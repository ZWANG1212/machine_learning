## P17 Learning with large datasets

**Stochastic gradient descent 随机梯度下降**

当训练集非常大时，梯度下降算法的计算量就会非常大，因为每次迭代都要用到所有的样本，这种梯度下降算法也叫批量梯度下降(Batch gradient descent, BGD)。

**Batch gradient descent**

$$
J_{train}(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})^2
$$

Repeat{
$$
\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)}) x_j^{(i)}
$$
   (for every $j$ = 0, ..., n)
}

**Stochastic gradient descent**

$$
cost(\theta, (x^{(i)}), y^{(i)}) = \frac{1}{2}(h_{\theta}(x^{(i)})-y^{(i)})^2
$$

$$
J_{train}(\theta)=\frac{1}{m}\sum_{i=1}^m cost(\theta, (x^{(i)}), y^{(i)})
$$

1. Randomly shuffle (reorder) training examples
2. Repeat{
for $i$ := 1, ..., m{
$$
\theta_j := \theta_j - \alpha (h_{\theta}(x^{(i)})-y^{(i)}) x_j^{(i)}
$$
  (for every $j$ = 0, ..., n)
 }
}

每次梯度迭代只用了一个训练样本，然后迭代样本总数量m次。

**Mini-batch gradient descent**

Batch gradient descent: Use all $m$ examples in each iteration. 收敛很好，但计算量很大。

Stochastic gradient descent: Use 1 example in each iteration. 收敛过程中容易受单个异常值影响。

Mini-batch gradient descent: Use $b$ examples in each iteration. 这个算法是BGD和SGD的折中，既可以很快，又可以消除收敛过程中单个异常值的影响。

$b$ = mini-batch, 通常b=10, 一般取2～100之间。

**Mini-batch gradient descent algorithm**

1. Say b = 10, m = 1000
2. Repeat{

for i = 1, 11, 21, 31, …, 991 {

$$
\theta_j := \theta_j - \alpha \frac{1}{10} \sum_{k=i}^{i+9}(h_{\theta}(x^{(k)})-y^{(k)})x_j^{(k)}
$$
   (for every $j$ = 0, ..., n )
  } 	
 }

**Checking for convergence 检查收敛性**

Batch gradient descent: 

​	Plot $J_{train}(\theta)$ as a function of the number of iterations of gradient descent

$$
J_{train}(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})^2
$$

Stochastic gradient descent:

​	$cost(\theta, (x^{(i)}, y^{(i)})) = \frac{1}{2}(h_{\theta}(x^{(i)})-y^{(i)})^2$

​	During learning, compute $cost(\theta, (x^{(i)}, y^{(i)}))$ before updating $\theta$ using ($x^{(i)}, y^{(i)}$).

​	Every 1000 iterations (say), plot $cost(\theta, (x^{(i)}, y^{(i)}))$ averaged over the last 1000 examples processed by algorithm.

如果画出的代价函数不收敛，而是发散的，有可能是 learning rate $\alpha$ 太大了，也可以让学习速率随着迭代次数增加而逐渐减小，比如：
$$
\alpha = \frac{const1}{iteration Number + const2}
$$
但这样做又多了两个参数不好确定，实际上一般还是把学习速率设置为常数。

另外，随机梯度下降算法最后不会收敛到全局最优，而是在最优点附近不断徘徊，学习速率小的话，可能更接近于最优点。

**Online learning**

比如有一个网站，拥有不断进入网站的用户流以及所产生的数据流，在线学习算法可以从数据流中学习用户的偏好，然后优化一些决策，比如商品价格，推荐的商品，新闻等等。

Features $x$ capture properties of user, we may want to learn $p(y=1|x;\theta)$ to optimize price. 学习价格在多少时，用户会购买商品(y=1)的概率比较大。

比如应用逻辑回归模型：

Repeat forever {

​	Get (x, y) corresponding to user.

​	Update $\theta$ using (x, y):

​		$\theta_j := \theta_j - \alpha (h_{\theta}(x)-y)x_j$

​		(j=0,1,…n)

}

这种算法可以及时根据用户的偏好变化调整价格等决策。

**Map-reduce**

应用于随机梯度下降不能解决的更大规模数据问题。

比如有一个Batch gradient descent批量梯度下降计算任务：
$$
\theta_j := \theta_j - \alpha \frac{1}{400} \sum_{i=1}^400 (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}
$$
假如我们有4台计算机，通过Map-reduce思想，将计算任务分为4份：

Machine 1: Use $(x^{(1)}, y^{(1)}), ..., (x^{(100)}, y^{(100)})​$

$$
temp_j^{(1)} = \sum_{i=1}^100 (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}
$$

Machine 2: Use $(x^{(101)}, y^{(101)}), ..., (x^{(200)}, y^{(200)})​$

$$
temp_j^{(2)} = \sum_{i=101}^200 (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}
$$

Machine 3: Use $(x^{(201)}, y^{(201)}), ..., (x^{(300)}, y^{(300)})​$

$$
temp_j^{(3)} = \sum_{i=201}^300 (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}
$$

Machine 4: Use $(x^{(301)}, y^{(301)}), ..., (x^{(400)}, y^{(400)})​$

$$
temp_j^{(4)} = \sum_{i=301}^400 (h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}
$$

然后由中心服务器整合各部分计算结果：
$$
\theta_j := \theta_j - \alpha \frac{1}{400}(temp_j^{(1)} + temp_j^{(2)} + temp_j^{(3)}+ temp_j^{(4)})
$$
$(j=0,..., n)$

这样就将运算速度提高了4倍

**Map-reduce and summation over the training set**

Many learning algorithms can be expressed as computing sums of functions over the training set. 

## P18

Photo OCR (Optical Character Recognition) pipeline

Image -> Text detection (sliding window detection) -> Character segmentation (sliding window) -> Character recognition

一个最有效地得到高性能机器学习系统的方法是使用一个低偏差机器学习算法，并且使用庞大的训练集去训练它。

**Ceiling analysis 上限分析**

度量系统pipeline每个模块的准确度，找出哪个模块有潜力 可以最大化提升性能。

